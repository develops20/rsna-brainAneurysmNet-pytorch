{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"competition","sourceId":99552,"databundleVersionId":13441085},{"sourceType":"datasetVersion","sourceId":12780021,"datasetId":8079690,"databundleVersionId":13404554}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ====================================================\n# CELL 1: IMPORTS & CONFIG\n# ====================================================\n\nimport os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport timm\nimport cv2\nimport pydicom\nimport nibabel as nib\nfrom scipy import ndimage\nfrom scipy.ndimage import label, center_of_mass\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport kaggle_evaluation.rsna_inference_server\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-07T07:19:02.463395Z","iopub.execute_input":"2025-09-07T07:19:02.463670Z","iopub.status.idle":"2025-09-07T07:19:47.422002Z","shell.execute_reply.started":"2025-09-07T07:19:02.463621Z","shell.execute_reply":"2025-09-07T07:19:47.421440Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n  data = fetch_version_info()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Competition Configuration\nclass Config:\n    # Paths\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    SEGMENTATION_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/segmentations/'\n    \n    # Stage 2 Configuration\n    ROI_SIZE = (224, 224)\n    ROIS_PER_SERIES = 5\n    BATCH_SIZE = 2\n    EPOCHS = 3\n    LEARNING_RATE = 1e-4\n    N_FOLDS = 1\n    \n    # Competition constants\n    ID_COL = 'SeriesInstanceUID'\n    LABEL_COLS = [\n        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n    ]\n    \n    # Device and training\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MIXED_PRECISION = True\n    STAGE2_CACHE_DIR = '/kaggle/working/stage2_cache'\n    # Optional: reuse Stage 1 external cache volumes directly for exact preprocessing parity\n    # STAGE1_EXTERNAL_CACHE_DIR = '/kaggle/input/rsna2025-v2-intracranial-aneurysm-detection-nb153/stage1_AneurysmNet_prebuilt_v2'\n    \n    # Debug\n    DEBUG_MODE = False\n    DEBUG_SAMPLES = 0\n    # External ROI/cache reuse (Kaggle dataset with precomputed outputs)\n    # ROIS_EXTERNAL_DIR = '/kaggle/input/rsna2025-brainnet-aneurysm-rois/rois'\n    # STAGE2_CACHE_EXTERNAL_DIR = '/kaggle/input/rsna2025-brainnet-aneurysm-rois/stage2_cache'\n    # Cache/throughput\n    REUSE_EXISTING_ROIS = True  # if cached training_df exists, reuse to skip long ROI extraction\n    # Direct volume mode (top_example-style) using Stage 0 32x384x384 volumes\n    DIRECT_VOLUME_MODE = True\n    # STAGE0_PREBUILT_ROOT = '/kaggle/input/rsna2025-v2-intracranial-aneurysm-detection-nb153/stage1_AneurysmNet_prebuilt_v2'\n    NUM_WORKERS = 6\n    PREFETCH_FACTOR = 6\n    PIN_MEMORY = True\n    PERSISTENT_WORKERS = True\n    # Caching\n    CACHE_VOLUMES = True\n    CACHE_DIR = '/kaggle/working/stage2_cache_vols'\n    CACHE_DTYPE = 'uint8'  # 'uint8' (~4.7MB/series) or 'float16' (~9.4MB/series)\n    CACHE_MAX_GB = 20.0    # soft cap; skip saving when exceeded\n    CACHE_VERBOSE = False  # set True to log cache hits/writes/skips\n\nprint(f\"‚úÖ Configuration loaded - Device: {Config.DEVICE}\")\n\n# Speed-friendly backend settings\ntry:\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.set_float32_matmul_precision('high')\nexcept Exception:\n    pass\n\n# ====================================================\n# CELL 1.1: LIGHTWEIGHT DICOM PREPROCESSOR (32x384x384)\n# ====================================================\n\nclass DICOMPreprocessorKaggle:\n    \"\"\"Minimal, memory-safe DICOM ‚Üí (32,384,384) volume preprocessor (offline, no deps).\"\"\"\n    def __init__(self, target_shape=(32, 384, 384)):\n        self.target_depth, self.target_height, self.target_width = target_shape\n\n    def process_series(self, series_path: str) -> np.ndarray:\n        # Cache check\n        if getattr(Config, 'CACHE_VOLUMES', False):\n            try:\n                os.makedirs(Config.CACHE_DIR, exist_ok=True)\n                sid = os.path.basename(series_path.rstrip('/'))\n                cache_base = os.path.join(Config.CACHE_DIR, f\"{sid}_32x384x384\")\n                # Try uint8 then float16 if present\n                for suffix in ['_u8.npy', '_f16.npy']:\n                    cache_path_try = cache_base + suffix\n                    if os.path.exists(cache_path_try):\n                        cached = np.load(cache_path_try, mmap_mode='r')\n                        vol = cached.astype(np.float32)\n                        if cached.dtype == np.uint8:\n                            vol = vol / 255.0\n                        if getattr(Config, 'CACHE_VERBOSE', False):\n                            print(f\"[CACHE] hit: {os.path.basename(cache_path_try)}\")\n                        return vol\n            except Exception:\n                cache_path = None\n        # Collect DICOMs\n        dicoms = []\n        for root, _, files in os.walk(series_path):\n            for f in files:\n                if f.endswith('.dcm'):\n                    try:\n                        ds = pydicom.dcmread(os.path.join(root, f), force=True)\n                        if hasattr(ds, 'PixelData'):\n                            dicoms.append(ds)\n                    except Exception:\n                        continue\n        if len(dicoms) == 0:\n            return np.zeros((self.target_depth, self.target_height, self.target_width), dtype=np.float32)\n\n        # Sort by patient-space normal if possible, else by InstanceNumber\n        try:\n            orient = np.array(dicoms[0].ImageOrientationPatient, dtype=np.float32)\n            rowv, colv = orient[:3], orient[3:]\n            normal = np.cross(rowv, colv)\n            def sort_key(ds):\n                ipp = np.array(getattr(ds, 'ImagePositionPatient', [0,0,0]), dtype=np.float32)\n                return float(np.dot(ipp, normal))\n            dicoms = sorted(dicoms, key=sort_key)\n        except Exception:\n            dicoms = sorted(dicoms, key=lambda ds: getattr(ds, 'InstanceNumber', 0))\n\n        base_h = int(getattr(dicoms[0], 'Rows', 256))\n        base_w = int(getattr(dicoms[0], 'Columns', 256))\n        c, w = 300.0, 700.0\n        lo, hi = c - w/2.0, c + w/2.0\n        modality = (getattr(dicoms[0], 'Modality', '') or '').upper()\n\n        slices = []\n        for ds in dicoms:\n            try:\n                fr = ds.pixel_array\n            except Exception:\n                continue\n            if fr.ndim >= 3:\n                h, w2 = fr.shape[-2], fr.shape[-1]\n                frames = fr.reshape(int(np.prod(fr.shape[:-2])), h, w2)\n            else:\n                frames = fr[np.newaxis, ...]\n            for sl in frames:\n                sl = sl.astype(np.float32)\n                if getattr(ds, 'PhotometricInterpretation', 'MONOCHROME2') == 'MONOCHROME1':\n                    sl = sl.max() - sl\n                slope = float(getattr(ds, 'RescaleSlope', 1.0)); intercept = float(getattr(ds, 'RescaleIntercept', 0.0))\n                sl = sl * slope + intercept\n                if sl.shape != (base_h, base_w):\n                    sl = cv2.resize(sl, (base_w, base_h))\n                if modality == 'CT':\n                    s = np.clip(sl, lo, hi)\n                    s = (s - lo) / (hi - lo + 1e-6)\n                else:\n                    mean = float(sl.mean()); std = float(sl.std() + 1e-6)\n                    s = (sl - mean) / std; zc = 3.0\n                    s = np.clip(s, -zc, zc); s = (s + zc) / (2.0*zc)\n                slices.append(s.astype(np.float32))\n\n        volf = np.stack(slices, axis=0) if slices else np.zeros((1, base_h, base_w), dtype=np.float32)\n        D = volf.shape[0]\n        idx = np.linspace(0, max(D-1,0), num=self.target_depth).astype(int) if D>0 else np.zeros(self.target_depth, dtype=int)\n        vT = volf[idx]\n        out = np.empty((self.target_depth, self.target_height, self.target_width), dtype=np.float32)\n        for i in range(self.target_depth):\n            out[i] = cv2.resize(vT[i], (self.target_width, self.target_height))\n        p1, p99 = np.percentile(out, [1, 99])\n        if p99 > p1:\n            out = np.clip(out, p1, p99)\n            out = (out - p1) / (p99 - p1 + 1e-8)\n        out = np.nan_to_num(out, nan=0.0, posinf=1.0, neginf=0.0)\n        # Save cache using configured dtype\n        try:\n            if getattr(Config, 'CACHE_VOLUMES', False):\n                # Respect soft size cap\n                try:\n                    total_bytes = 0\n                    for f in os.listdir(Config.CACHE_DIR):\n                        fp = os.path.join(Config.CACHE_DIR, f)\n                        try:\n                            total_bytes += os.path.getsize(fp)\n                        except Exception:\n                            pass\n                    if total_bytes > Config.CACHE_MAX_GB * (1024**3):\n                        if getattr(Config, 'CACHE_VERBOSE', False):\n                            print(\"[CACHE] size cap reached; skip saving\")\n                        raise RuntimeError('Cache size cap reached, skip saving')\n                except Exception:\n                    pass\n                # Ensure cache dir exists and choose dtype and filename suffix\n                os.makedirs(Config.CACHE_DIR, exist_ok=True)\n                sid = os.path.basename(series_path.rstrip('/'))\n                cache_base = os.path.join(Config.CACHE_DIR, f\"{sid}_32x384x384\")\n                dtype_choice = getattr(Config, 'CACHE_DTYPE', 'float16')\n                if dtype_choice == 'uint8':\n                    # Quantize to 8-bit [0,255] after normalization\n                    arr8 = (out * 255.0).round().astype(np.uint8)\n                    # Optional quick verification to ensure acceptable quantization error\n                    try:\n                        diff_max = float(np.max(np.abs(out - (arr8.astype(np.float32) / 255.0))))\n                        if diff_max > 0.02:  # fallback threshold for safety\n                            # Fallback to float16 if quantization too lossy\n                            np.save(cache_base + '_f16.npy', out.astype(np.float16), allow_pickle=False)\n                            if getattr(Config, 'CACHE_VERBOSE', False):\n                                print(f\"[CACHE] wrote f16 (fallback): {os.path.basename(cache_base)}_f16.npy\")\n                        else:\n                            np.save(cache_base + '_u8.npy', arr8, allow_pickle=False)\n                            if getattr(Config, 'CACHE_VERBOSE', False):\n                                print(f\"[CACHE] wrote u8: {os.path.basename(cache_base)}_u8.npy (max_err={diff_max:.4f})\")\n                    except Exception:\n                        np.save(cache_base + '_u8.npy', arr8, allow_pickle=False)\n                        if getattr(Config, 'CACHE_VERBOSE', False):\n                            print(f\"[CACHE] wrote u8 (no-check): {os.path.basename(cache_base)}_u8.npy\")\n                else:\n                    np.save(cache_base + '_f16.npy', out.astype(np.float16), allow_pickle=False)\n                    if getattr(Config, 'CACHE_VERBOSE', False):\n                        print(f\"[CACHE] wrote f16: {os.path.basename(cache_base)}_f16.npy\")\n        except Exception:\n            pass\n        return out.astype(np.float32)\n\ndef process_dicom_series_safe(series_path: str, target_shape=(32,384,384)) -> np.ndarray:\n    try:\n        pre = DICOMPreprocessorKaggle(target_shape)\n        return pre.process_series(series_path)\n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n        try:\n            gc.collect()\n        except Exception:\n            pass\n\n\n# ====================================================\n# CELL 2: DATA LOADING & ROI EXTRACTION\n# ====================================================\n\nclass Simple3DSegmentationNet(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.dummy = nn.Identity()\n    def forward(self, x):\n        return self.dummy(x)\n\nclass Stage1Predictor:\n    def __init__(self, *args, **kwargs):\n        pass\n    def predict_segmentation_with_volume(self, series_path):\n        # Not used in direct-volume path; kept minimal for compatibility\n        return np.zeros((1,1,1), dtype=np.float32), np.zeros((32,384,384), dtype=np.float32)\n\nclass ROIExtractor:\n    \"\"\"Research-backed ROI extraction with adaptive count and quality filtering\"\"\"\n    def __init__(self, stage1_predictor, roi_size=(224, 224)):\n        self.stage1_predictor = stage1_predictor\n        self.roi_size = roi_size\n        self.processor = SimpleDICOMProcessor()\n\n        # Research-backed thresholds\n        # Relaxed thresholds to avoid over-pruning when Stage 1 is weak\n        self.min_confidence_threshold = 0.15\n        self.high_confidence_threshold = 0.5\n        self.max_rois_per_series = getattr(Config, 'ROIS_PER_SERIES', 3)\n        # Post-process controls\n        self.border_margin = 2            # suppress edge activations near skull\n        self.min_region_size = 6         # minimum connected component size (pixels)\n        self.morph_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n\n    def extract_top3_rois(self, series_path):\n        \"\"\"Extract 0-5 ROIs based on segmentation quality (research-backed)\"\"\"\n        # Cache ROI results per series to avoid recomputation\n        try:\n            # Prefer external cache dir when available\n            cache_root = getattr(Config, 'STAGE2_CACHE_EXTERNAL_DIR', '')\n            if not isinstance(cache_root, str) or not os.path.isdir(cache_root):\n                cache_root = Config.STAGE2_CACHE_DIR\n            os.makedirs(cache_root, exist_ok=True)\n            sid = os.path.basename(series_path)\n            cache_path = os.path.join(cache_root, f\"{sid}_rois.npy\")\n            if os.path.exists(cache_path):\n                arr = np.load(cache_path, allow_pickle=True)\n                return list(arr)\n        except Exception:\n            cache_path = None\n        rois = self.extract_adaptive_rois(series_path)\n        try:\n            if cache_path is not None:\n                np.save(cache_path, np.array(rois, dtype=object), allow_pickle=True)\n        except Exception:\n            pass\n        return rois\n\n    def extract_adaptive_rois(self, series_path):\n        \"\"\"Extract 0-5 ROIs based on segmentation quality (research-backed)\"\"\"\n        try:\n            print(f\"üîç DEBUG: Quality-based ROI extraction for {os.path.basename(series_path)}\")\n            \n            # Get Stage 1 seg mask and the preprocessed volume (avoid reloading original DICOMs here)\n            seg_mask, original_volume = self.stage1_predictor.predict_segmentation_with_volume(series_path)\n            print(f\"üîç DEBUG: Segmentation mask shape: {seg_mask.shape}; Volume shape: {original_volume.shape}\")\n            \n            # STEP 1: Assess overall segmentation quality\n            seg_quality = self._assess_segmentation_quality(seg_mask)\n            print(f\"üîç DEBUG: Segmentation quality score: {seg_quality:.3f}\")\n            \n            # STEP 2: If segmentation is poor, still attempt candidate extraction; fallback only if none\n            low_quality = seg_quality < self.min_confidence_threshold\n            if low_quality:\n                print(f\"üîç DEBUG: Low segmentation quality ({seg_quality:.3f} < {self.min_confidence_threshold}), attempting candidate extraction anyway\")\n            \n            # STEP 4: Extract ROIs with confidence-based filtering\n            roi_candidates = self._find_quality_based_rois(seg_mask, original_volume)\n            \n            if low_quality and not roi_candidates:\n                print(\"üîç DEBUG: No candidates under low-quality mask, using volume-based fallback\")\n                return self._get_quality_fallback_rois_from_volume(original_volume, self.max_rois_per_series)\n\n            # STEP 5: Adaptive ROI count\n            selected_rois = self._select_adaptive_rois(roi_candidates, seg_quality, original_volume)\n            \n            print(f\"üîç DEBUG: Selected {len(selected_rois)} ROIs based on quality assessment\")\n            return selected_rois\n            \n        except Exception as e:\n            print(f\"‚ùå Error in quality-based ROI extraction: {e}\")\n            return self._get_emergency_fallback_rois()\n    \n    def _assess_segmentation_quality(self, seg_mask):\n        \"\"\"Assess segmentation quality using connected components and border penalties.\"\"\"\n        try:\n            D, H, W = seg_mask.shape\n            largest_area_frac = 0.0\n            largest_mean_conf = 0.0\n            total_components = 0\n            border_touch_penalty = 0.0\n\n            for z in range(D):\n                sm = seg_mask[z]\n                # suppress borders\n                sm_proc = sm.copy()\n                sm_proc[:self.border_margin, :] = 0\n                sm_proc[-self.border_margin:, :] = 0\n                sm_proc[:, :self.border_margin] = 0\n                sm_proc[:, -self.border_margin:] = 0\n\n               # Adaptive thresholding based on actual max values\n                max_val = float(sm_proc.max())\n                if max_val > 0.3:\n                    thr = max(0.05, 0.3 * max_val)\n                elif max_val > 0.1:\n                    thr = max(0.03, 0.4 * max_val)\n                else:\n                    thr = max(0.02, 0.5 * max_val)\n                binmask = (sm_proc > thr).astype(np.uint8)\n                if binmask.max() == 0:\n                    continue\n                # small opening to remove speckle\n                binmask = cv2.morphologyEx(binmask, cv2.MORPH_OPEN, self.morph_kernel)\n\n                labeled, n = label(binmask)\n                if n == 0:\n                    continue\n                total_components += int(n)\n\n                # evaluate components\n                for comp_id in range(1, n + 1):\n                    comp = (labeled == comp_id)\n                    comp_size = int(comp.sum())\n                    if comp_size < self.min_region_size:\n                        continue\n                    mean_conf = float(sm[comp].mean())\n                    area_frac = comp_size / float(H * W)\n                    if area_frac > largest_area_frac:\n                        largest_area_frac = area_frac\n                    if mean_conf > largest_mean_conf:\n                        largest_mean_conf = mean_conf\n\n                    # simple border-touch penalty if component abuts image edge\n                    ys, xs = np.where(comp)\n                    if ys.size > 0:\n                        if (ys.min() <= self.border_margin or ys.max() >= H - self.border_margin - 1 or\n                            xs.min() <= self.border_margin or xs.max() >= W - self.border_margin - 1):\n                            border_touch_penalty += 0.02\n\n            # compose quality score\n            area_score = min(largest_area_frac / 0.02, 1.0)  # cap around ~2% of slice (aneurysm-sized)\n            comp_penalty = min(0.1, 0.0015 * total_components) + min(0.1, border_touch_penalty)\n            quality_score = max(0.0, 0.6 * largest_mean_conf + 0.4 * area_score - comp_penalty)\n\n            # robust floor based on global mask stats to avoid spurious 0.0 quality\n            max_val = float(seg_mask.max())\n            mean_val = float(seg_mask.mean())\n            if max_val >= 0.55:\n                quality_score = max(quality_score, 0.35)\n            elif max_val >= 0.45:\n                quality_score = max(quality_score, 0.25)\n            elif mean_val >= 0.25:\n                quality_score = max(quality_score, 0.22)\n\n            return float(quality_score)\n        except Exception:\n            return 0.1\n    \n    def _find_quality_based_rois(self, seg_mask, original_volume):\n        \"\"\"Find ROI candidates with confidence scores (no hardcoded count)\"\"\"\n        print(\"üîç DEBUG: Finding quality-based ROI candidates...\")\n        \n        # Resize segmentation mask to match original volume\n        if seg_mask.shape != original_volume.shape:\n            print(\"üîç DEBUG: Resizing segmentation mask with cv2...\")\n            seg_mask_resized = np.zeros(original_volume.shape, dtype=np.float32)\n            for i in range(min(seg_mask.shape[0], original_volume.shape[0])):\n                if i < seg_mask.shape[0]:\n                    resized_slice = cv2.resize(\n                        seg_mask[i],\n                        (original_volume.shape[2], original_volume.shape[1])\n                    )\n                    seg_mask_resized[i] = resized_slice\n        else:\n            seg_mask_resized = seg_mask\n        \n        # 3D peak proposals first (relative peak logic; does not lower thresholds)\n        roi_candidates = self._proposals_from_3d_peaks(seg_mask_resized)\n        if len(roi_candidates) == 0:\n            # Fall back to 2D slice-wise CC method\n            roi_candidates = []\n        \n        H, W = original_volume.shape[1], original_volume.shape[2]\n        for slice_idx in range(seg_mask_resized.shape[0]):\n            slice_mask = seg_mask_resized[slice_idx].copy()\n\n            # Suppress borders to avoid skull/edge activations\n            slice_mask[:self.border_margin, :] = 0\n            slice_mask[-self.border_margin:, :] = 0\n            slice_mask[:, :self.border_margin] = 0\n            slice_mask[:, -self.border_margin:] = 0\n\n            # Adaptive dynamic threshold tied to local max (aligned with quality assessment)\n            max_val = float(slice_mask.max())\n            if max_val > 0.2:\n                thr = max(self.min_confidence_threshold, 0.25 * max_val)\n            elif max_val > 0.1:\n                thr = max(0.03, 0.30 * max_val)\n            else:\n                thr = max(0.02, 0.25 * max_val)\n            high_conf_regions = (slice_mask > thr).astype(np.uint8)\n            if high_conf_regions.max() == 0:\n                # Percentile-based fallback with small dilation to form blobs\n                p90 = float(np.percentile(slice_mask, 90))\n                if p90 > 0:\n                    mask_peaks = (slice_mask >= p90).astype(np.uint8)\n                    # small dilation to merge nearby high pixels\n                    mask_peaks = cv2.dilate(mask_peaks, self.morph_kernel, iterations=1)\n                    labeled_regions, num_regions = label(mask_peaks)\n                    for region_id in range(1, num_regions + 1):\n                        region_mask = (labeled_regions == region_id)\n                        region_size = int(region_mask.sum())\n                        if region_size < 3:\n                            continue\n                        ys, xs = np.where(region_mask)\n                        if ys.size == 0:\n                            continue\n                        # Skip borders\n                        if (ys.min() <= self.border_margin or ys.max() >= H - self.border_margin - 1 or\n                            xs.min() <= self.border_margin or xs.max() >= W - self.border_margin - 1):\n                            continue\n                        com = center_of_mass(region_mask)\n                        y, x = int(com[0]), int(com[1])\n                        region_confidence = float(slice_mask[region_mask].mean())\n                        roi_candidates.append({\n                            'slice_idx': slice_idx,\n                            'y': y,\n                            'x': x,\n                            'confidence': region_confidence,\n                            'region_size': region_size\n                        })\n                continue\n            # Apply opening only if region is sufficiently large; avoid eroding tiny blobs\n            if int(high_conf_regions.sum()) > 50:\n                high_conf_regions = cv2.morphologyEx(high_conf_regions, cv2.MORPH_OPEN, self.morph_kernel)\n\n            labeled_regions, num_regions = label(high_conf_regions)\n            for region_id in range(1, num_regions + 1):\n                region_mask = (labeled_regions == region_id)\n                region_size = int(region_mask.sum())\n                if region_size < self.min_region_size:\n                    continue\n                ys, xs = np.where(region_mask)\n                if ys.size == 0:\n                    continue\n                # Skip border-touching components\n                if (ys.min() <= self.border_margin or ys.max() >= H - self.border_margin - 1 or\n                    xs.min() <= self.border_margin or xs.max() >= W - self.border_margin - 1):\n                    continue\n\n                com = center_of_mass(region_mask)\n                y, x = int(com[0]), int(com[1])\n                region_confidence = float(slice_mask[region_mask].mean())\n\n                roi_candidates.append({\n                    'slice_idx': slice_idx,\n                    'y': y,\n                    'x': x,\n                    'confidence': region_confidence,\n                    'region_size': region_size\n                })\n        \n        # Sort by confidence (descending)\n        if not roi_candidates:\n            # Volume-wise peak fallback: pick top maxima per slice (excluding borders)\n            print(\"üîç DEBUG: No ROI components found; using volume-wise peak fallback\")\n            D = seg_mask_resized.shape[0]\n            peak_candidates = []\n            for z in range(D):\n                m = seg_mask_resized[z].copy()\n                # suppress borders\n                m[:self.border_margin, :] = 0\n                m[-self.border_margin:, :] = 0\n                m[:, :self.border_margin] = 0\n                m[:, -self.border_margin:] = 0\n                yx = np.unravel_index(np.argmax(m), m.shape)\n                y, x = int(yx[0]), int(yx[1])\n                conf = float(m[y, x])\n                if conf > 0:\n                    peak_candidates.append({\n                        'slice_idx': z,\n                        'y': y,\n                        'x': x,\n                        'confidence': conf,\n                        'region_size': 1\n                    })\n            # Keep strongest few peaks across volume\n            peak_candidates.sort(key=lambda c: c['confidence'], reverse=True)\n            roi_candidates.extend(peak_candidates[: max( self.max_rois_per_series * 3, 6)])\n\n        roi_candidates.sort(key=lambda x: x['confidence'], reverse=True)\n        \n        print(f\"üîç DEBUG: Found {len(roi_candidates)} ROI candidates\")\n        return roi_candidates\n\n    def _proposals_from_3d_peaks(self, seg_mask_zyx: np.ndarray):\n        \"\"\"3D local-max proposals with seeded relative growth (no absolute threshold lowering).\"\"\"\n        try:\n            D, H, W = seg_mask_zyx.shape\n            # Light 3D smoothing to stabilize local maxima\n            try:\n                sm = ndimage.gaussian_filter(seg_mask_zyx.astype(np.float32), sigma=0.75)\n            except Exception:\n                sm = seg_mask_zyx.astype(np.float32)\n            # 3D local maxima via maximum filter\n            footprint = np.ones((3,3,3), dtype=np.uint8)\n            max_f = ndimage.maximum_filter(sm, footprint=footprint, mode='nearest')\n            peaks = (sm == max_f)\n            # Suppress borders\n            b = self.border_margin\n            if b > 0:\n                peaks[:, :b, :] = False; peaks[:, -b:, :] = False\n                peaks[:, :, :b] = False; peaks[:, :, -b:] = False\n            coords = np.argwhere(peaks)\n            if coords.shape[0] == 0:\n                return []\n            # Rank peaks by value and keep top-K to control cost\n            values = sm[peaks]\n            order = np.argsort(values)[::-1]\n            top_k = min(64, order.size)\n            selected = coords[order[:top_k]]\n            # Non-maximum suppression by 3D distance\n            kept = []\n            min_dist = 4.0\n            for (cz, cy, cx) in selected:\n                if any(((cz-kz)**2 + (cy-ky)**2 + (cx-kx)**2) ** 0.5 < min_dist for kz,ky,kx in kept):\n                    continue\n                kept.append((int(cz), int(cy), int(cx)))\n                if len(kept) >= 64:\n                    break\n            # Seeded relative growth\n            proposals = []\n            for cz, cy, cx in kept:\n                peak = float(sm[cz, cy, cx])\n                if peak <= 0:\n                    continue\n                rel_thr = max(0.6*peak, 1e-6)  # relative to each peak\n                # collect voxels that descend from the peak (thresholded region)\n                region = sm >= rel_thr\n                labeled, num = ndimage.label(region)\n                cid = int(labeled[cz, cy, cx])\n                if cid == 0:\n                    continue\n                comp = (labeled == cid)\n                size = int(comp.sum())\n                if size < self.min_region_size:\n                    continue\n                # score = peak * mean(comp)\n                conf = peak * float(sm[comp].mean() + 1e-6)\n                # project to a representative slice (peak slice)\n                ys, xs = np.where(comp[cz])\n                if ys.size == 0:\n                    # fallback to COM over full comp\n                    zc, yc, xc = ndimage.center_of_mass(comp)\n                    zc = int(round(zc)); yc = int(round(yc)); xc = int(round(xc))\n                    if yc <= self.border_margin or yc >= H - self.border_margin - 1 or xc <= self.border_margin or xc >= W - self.border_margin - 1:\n                        continue\n                    proposals.append({\n                        'slice_idx': int(zc),\n                        'y': int(yc),\n                        'x': int(xc),\n                        'confidence': float(conf),\n                        'region_size': size,\n                    })\n                else:\n                    y = int(ys.mean()); x = int(xs.mean())\n                    if y <= self.border_margin or y >= H - self.border_margin - 1 or x <= self.border_margin or x >= W - self.border_margin - 1:\n                        continue\n                    proposals.append({\n                        'slice_idx': int(cz),\n                        'y': y,\n                        'x': x,\n                        'confidence': float(conf),\n                        'region_size': size,\n                    })\n            proposals.sort(key=lambda c: c['confidence'], reverse=True)\n            return proposals\n        except Exception:\n            return []\n    \n    def _select_adaptive_rois(self, roi_candidates, seg_quality, original_volume):\n        \"\"\"Adaptively select ROIs based on segmentation quality (research-backed)\"\"\"\n        if not roi_candidates:\n            print(\"üîç DEBUG: No candidates found, using fallback\")\n            return self._get_quality_fallback_rois_from_volume(original_volume)\n        \n        # Adaptive selection based on segmentation quality\n        if seg_quality >= self.high_confidence_threshold:\n            max_rois = self.max_rois_per_series\n            min_confidence = 0.3\n        elif seg_quality >= self.min_confidence_threshold + 0.2:\n            max_rois = self.max_rois_per_series\n            min_confidence = 0.2\n        else:\n            max_rois = self.max_rois_per_series\n            min_confidence = 0.05\n        \n        # Filter and select ROIs\n        filtered = [c for c in roi_candidates if c['confidence'] >= min_confidence]\n        selected_candidates = filtered[:max_rois]\n        # If not enough, top-off with next best candidates\n        if len(selected_candidates) < max_rois:\n            for c in roi_candidates:\n                if c in selected_candidates:\n                    continue\n                selected_candidates.append(c)\n                if len(selected_candidates) >= max_rois:\n                    break\n        \n        # Convert to ROI format\n        rois = []\n        for i, candidate in enumerate(selected_candidates):\n            roi_patch = self._extract_roi_patch(\n                original_volume,\n                candidate['slice_idx'], \n                candidate['y'], \n                candidate['x']\n            )\n            \n            rois.append({\n                'roi_image': roi_patch,\n                'slice_idx': candidate['slice_idx'],\n                'coordinates': (candidate['y'], candidate['x']),\n                'confidence': candidate['confidence'],\n                'roi_id': i\n            })\n        # Ensure at least max_rois via center-based fallback if still short\n        if len(rois) < self.max_rois_per_series:\n            needed = self.max_rois_per_series - len(rois)\n            center_fallbacks = self._get_quality_fallback_rois_from_volume(original_volume, needed)\n            rois.extend(center_fallbacks)\n        print(f\"üîç DEBUG: Adaptively selected {len(rois)} ROIs (quality: {seg_quality:.3f})\")\n        return rois[: self.max_rois_per_series]\n    \n    def _get_quality_fallback_rois(self, series_path, seg_mask):\n        \"\"\"Fallback for poor segmentation quality: generate multiple center-based ROIs\"\"\"\n        print(\"üîç DEBUG: Using quality-aware fallback (multi-center ROIs)\")\n        original_volume = self._load_efficient_volume(series_path)\n        return self._get_quality_fallback_rois_from_volume(original_volume, self.max_rois_per_series)\n\n    def _get_quality_fallback_rois_from_volume(self, original_volume, count: int = 3):\n        D, H, W = original_volume.shape\n        # Choose slice indices: center and quartiles\n        slices = sorted(set([D // 2, max(0, D // 4), min(D - 1, 3 * D // 4)]))\n        # Ensure desired count\n        while len(slices) < count:\n            # Add random slices if needed\n            slices.append(np.random.randint(0, D))\n            slices = list(dict.fromkeys(slices))\n        rois = []\n        cy, cx = H // 2, W // 2\n        for i, s in enumerate(slices[:count]):\n            roi_patch = self._extract_roi_patch(original_volume, s, cy, cx)\n            rois.append({\n                'roi_image': roi_patch,\n                'slice_idx': s,\n                'coordinates': (cy, cx),\n                'confidence': 0.2,\n                'roi_id': i\n            })\n        return rois\n    \n    def _get_simple_fallback_rois(self):\n        \"\"\"Simple fallback when no quality ROIs found\"\"\"\n        print(\"üîç DEBUG: Using simple fallback (single center ROI)\")\n        dummy_roi = np.random.random((*Config.ROI_SIZE, 3)).astype(np.float32)\n        return [{\n            'roi_image': dummy_roi,\n            'slice_idx': 25,\n            'coordinates': (128, 128),\n            'confidence': 0.1,\n            'roi_id': 0\n        }]\n    \n    def _get_emergency_fallback_rois(self):\n        \"\"\"Emergency fallback when everything fails\"\"\"\n        print(\"üîç DEBUG: Using emergency fallback ROI\")\n        dummy_roi = np.random.random((*Config.ROI_SIZE, 3)).astype(np.float32)\n        return [{\n            'roi_image': dummy_roi,\n            'slice_idx': 0,\n            'coordinates': (128, 128),\n            'confidence': 0.1,\n            'roi_id': 0\n        }]\n\n    \n    def _load_efficient_volume(self, series_path):\n        \"\"\"Load volume with smart distributed sampling to cover entire brain\"\"\"\n        try:\n            # Cache original volume slices to reduce repeated I/O\n            os.makedirs(Config.STAGE2_CACHE_DIR, exist_ok=True)\n            sid = os.path.basename(series_path)\n            vcache = os.path.join(Config.STAGE2_CACHE_DIR, f\"{sid}_vol.npy\")\n            if os.path.exists(vcache):\n                return np.load(vcache, allow_pickle=False)\n            dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n            pixel_arrays = []\n            \n            # SMART SAMPLING: Distribute 50 slices across entire volume\n            total_files = len(dicom_files)\n            if total_files > 50:\n                # Calculate step size to distribute slices evenly\n                step = total_files / 50\n                selected_indices = [int(i * step) for i in range(50)]\n                selected_files = [dicom_files[i] for i in selected_indices]\n                print(f\"üîç DEBUG: Smart sampling - selected {len(selected_files)} files from {total_files} total (every {step:.1f})\")\n            else:\n                selected_files = dicom_files\n                print(f\"üîç DEBUG: Using all {len(selected_files)} files (less than 50)\")\n            \n            for f in selected_files:\n                try:\n                    ds = pydicom.dcmread(os.path.join(series_path, f), force=True)\n                    if hasattr(ds, 'pixel_array'):\n                        arr = ds.pixel_array\n                        if arr.ndim == 2:\n                            pixel_arrays.append(arr)\n                except:\n                    continue\n            \n            if pixel_arrays:\n                # SMALLER target shape to reduce memory usage\n                target_shape = (256, 256)  # Reduced from (512, 512)\n                \n                resized_arrays = []\n                for arr in pixel_arrays:\n                    # Use cv2.resize instead of ndimage.zoom (more reliable)\n                    if arr.shape != target_shape:\n                        resized_arr = cv2.resize(arr.astype(np.float32), target_shape)\n                        resized_arrays.append(resized_arr)\n                    else:\n                        resized_arrays.append(arr.astype(np.float32))\n                \n                volume = np.stack(resized_arrays, axis=0)\n                \n                # Simple normalization\n                p1, p99 = np.percentile(volume, [1, 99])\n                volume = np.clip(volume, p1, p99)\n                volume = (volume - p1) / (p99 - p1 + 1e-8)\n                \n                try:\n                    np.save(vcache, volume.astype(np.float32), allow_pickle=False)\n                except Exception:\n                    pass\n                return volume\n            \n        except Exception as e:\n            print(f\"Error loading efficient volume: {e}\")\n        \n        # Fallback volume (matches our smart sampling approach)\n        return np.random.random((50, 256, 256)).astype(np.float32)\n\n    \n    def _extract_roi_patch(self, volume, slice_idx, center_y, center_x):\n        \"\"\"Extract ROI with adjacent-slice context as RGB channels (s-1, s, s+1).\"\"\"\n        D, H, W = volume.shape\n        s_indices = [max(0, slice_idx - 1), slice_idx, min(D - 1, slice_idx + 1)]\n        channels = []\n        half_size = Config.ROI_SIZE[0] // 2\n        for s in s_indices:\n            slice_data = volume[s]\n            h, w = slice_data.shape\n            y1 = max(0, center_y - half_size)\n            y2 = min(h, center_y + half_size)\n            x1 = max(0, center_x - half_size)\n            x2 = min(w, center_x + half_size)\n            patch = slice_data[y1:y2, x1:x2]\n            patch_resized = cv2.resize(patch, Config.ROI_SIZE)\n            channels.append(patch_resized)\n        patch_3ch = np.stack(channels, axis=2)\n        return patch_3ch\n    \n\ndef create_training_data(df, stage1_predictor):\n    \"\"\"Create training data. If DIRECT_VOLUME_MODE, bypass ROI extraction and use Stage0 32x384x384 volumes.\"\"\"\n    print(\"üîÑ Extracting ROIs for training data...\")\n    \n    # Direct volume mode: build dataframe pointing to Stage0 32x384x384 volumes\n    if getattr(Config, 'DIRECT_VOLUME_MODE', False):\n        print(\"‚úÖ DIRECT_VOLUME_MODE: using Stage 0 32x384x384 volumes (no ROI extraction)\")\n        # On-the-fly generation: do not depend on prebuilt volumes; dataset will read DICOMs\n        records = []\n        for _, row in df.iterrows():\n            sid = str(row[Config.ID_COL])\n            rec = {\n                'roi_id': f\"{sid}_vol32\",\n                'roi_path': '',\n                'series_id': sid,\n                'roi_confidence': 1.0,\n                'slice_idx': -1,\n            }\n            for col in Config.LABEL_COLS:\n                rec[col] = row[col]\n            records.append(rec)\n        training_df = pd.DataFrame(records)\n        print(f\"‚úÖ DIRECT_VOLUME_MODE: built {len(training_df)} samples from {len(df)} series\")\n        return training_df\n\n    # Reuse cached ROIs/training dataframe if available\n    cache_dir = 'rois'\n    os.makedirs(cache_dir, exist_ok=True)\n    cached_df_path_parquet = os.path.join(cache_dir, 'training_df.parquet')\n    external_cached_df_path = os.path.join(getattr(Config, 'ROIS_EXTERNAL_DIR', ''), 'training_df.parquet')\n    if Config.REUSE_EXISTING_ROIS:\n        # Prefer working cache\n        if os.path.exists(cached_df_path_parquet):\n            try:\n                cached = pl.read_parquet(cached_df_path_parquet).to_pandas()\n                if len(cached) > 0 and all(c in cached.columns for c in ['roi_path', 'roi_id', 'series_id'] + Config.LABEL_COLS):\n                    print(f\"‚úÖ Reusing cached training ROIs (working): {len(cached)} samples from {cached['series_id'].nunique()} series\")\n                    return cached\n            except Exception:\n                pass\n        # Fallback to external cache\n        if isinstance(external_cached_df_path, str) and len(external_cached_df_path) and os.path.exists(external_cached_df_path):\n            try:\n                cached = pl.read_parquet(external_cached_df_path).to_pandas()\n                if len(cached) > 0 and all(c in cached.columns for c in ['roi_path', 'roi_id', 'series_id'] + Config.LABEL_COLS):\n                    print(f\"‚úÖ Reusing cached training ROIs (external): {len(cached)} samples from {cached['series_id'].nunique()} series\")\n                    # Optionally copy into working for faster subsequent access\n                    try:\n                        pl.from_pandas(cached).write_parquet(cached_df_path_parquet)\n                    except Exception:\n                        pass\n                    return cached\n            except Exception:\n                pass\n        # If external parquet is missing but external ROI images exist, auto-build parquet\n        ext_dir = getattr(Config, 'ROIS_EXTERNAL_DIR', '')\n        if isinstance(ext_dir, str) and os.path.isdir(ext_dir):\n            try:\n                candidates = [f for f in os.listdir(ext_dir) if f.lower().endswith('.png')]\n                if len(candidates) > 0:\n                    print(f\"üß© Building training_df from external ROI images: {len(candidates)} files\")\n                    records = []\n                    label_cols = list(Config.LABEL_COLS)\n                    # Map labels by series_id for fast join\n                    df_labels = df[[Config.ID_COL] + label_cols].copy()\n                    df_labels[Config.ID_COL] = df_labels[Config.ID_COL].astype(str)\n                    label_map = df_labels.set_index(Config.ID_COL).to_dict('index')\n                    for fname in candidates:\n                        base = os.path.splitext(fname)[0]\n                        # Expect pattern: {series_id}_roi_{k}\n                        # Robust parse: split on '_roi_'\n                        if '_roi_' not in base:\n                            continue\n                        sid_part, roi_part = base.split('_roi_', 1)\n                        series_id = sid_part\n                        try:\n                            roi_id_int = int(roi_part)\n                        except Exception:\n                            roi_id_int = 0\n                        rec = {\n                            'roi_id': f\"{series_id}_roi_{roi_id_int}\",\n                            'roi_path': os.path.join(ext_dir, fname),\n                            'series_id': series_id,\n                            'roi_confidence': 0.2,\n                            'slice_idx': -1,\n                        }\n                        # Attach labels\n                        labs = label_map.get(series_id)\n                        if labs is None:\n                            # skip if label missing (should not happen for train)\n                            continue\n                        for col in label_cols:\n                            rec[col] = labs[col]\n                        records.append(rec)\n                    if records:\n                        training_df_ext = pd.DataFrame(records)\n                        print(f\"‚úÖ Reconstructed training ROIs (external): {len(training_df_ext)} samples from {training_df_ext['series_id'].nunique()} series\")\n                        try:\n                            pl.from_pandas(training_df_ext).write_parquet(cached_df_path_parquet)\n                        except Exception:\n                            pass\n                        return training_df_ext\n            except Exception:\n                pass\n    roi_extractor = ROIExtractor(stage1_predictor)\n    training_data = []\n    \n    os.makedirs('rois', exist_ok=True)\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting ROIs\"):\n        series_id = row[Config.ID_COL]\n        series_path = os.path.join(Config.SERIES_DIR, series_id)\n        \n        if not os.path.exists(series_path):\n            continue\n        \n        # Extract ROIs\n        rois = roi_extractor.extract_top3_rois(series_path)\n        \n        # Create training samples\n        for roi_data in rois:\n            roi_filename = f\"rois/{series_id}_roi_{roi_data['roi_id']}.png\"\n            \n            # Save ROI image\n            roi_image = (roi_data['roi_image'] * 255).astype(np.uint8)\n            Image.fromarray(roi_image).save(roi_filename)\n            \n            # Create training record\n            sample = {\n                'roi_id': f\"{series_id}_roi_{roi_data['roi_id']}\",\n                'roi_path': roi_filename,\n                'series_id': series_id,\n                'roi_confidence': roi_data['confidence'],\n                'slice_idx': roi_data['slice_idx']\n            }\n            \n            # Add all label columns\n            for col in Config.LABEL_COLS:\n                sample[col] = row[col]\n            \n            training_data.append(sample)\n    \n    training_df = pd.DataFrame(training_data)\n    print(f\"‚úÖ Created {len(training_df)} training samples from {len(df)} series\")\n    # Save for reuse next runs\n    try:\n        pl.from_pandas(training_df).write_parquet(cached_df_path_parquet)\n        print(f\"üíæ Saved training ROI dataframe ‚Üí {cached_df_path_parquet}\")\n    except Exception:\n        pass\n    \n    return training_df\n\nprint(\"‚úÖ Data loading and ROI extraction functions loaded\")\n\n# ====================================================\n# CELL 3: MODEL DEFINITION\n# ====================================================\n\nclass AneurysmClassificationDataset(Dataset):\n    \"\"\"Dataset for classification. In direct volume mode, builds 32x384x384 on-the-fly from DICOMs.\"\"\"\n    def __init__(self, df, mode='train'):\n        self.df = df\n        self.mode = mode\n        self.direct_volume = getattr(Config, 'DIRECT_VOLUME_MODE', False)\n        if self.direct_volume:\n            self.preprocessor = DICOMPreprocessorKaggle(target_shape=(32, 384, 384))\n            # Albumentations pipeline for 32-channel input: only Normalize + ToTensorV2\n            self.alb_transform = A.Compose([\n                A.Normalize(mean=0.0, std=1.0, max_pixel_value=1.0),\n                ToTensorV2(transpose_mask=False),\n            ])\n        else:\n            # ROI image pipeline (3-channel PNGs) - keep minimal and albumentations-based\n            if mode == 'train':\n                self.alb_transform = A.Compose([\n                    A.HorizontalFlip(p=0.5),\n                    A.VerticalFlip(p=0.5),\n                    A.Rotate(limit=15, p=0.5),\n                    A.ColorJitter(brightness=0.2, contrast=0.2, p=0.5),\n                    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n                    ToTensorV2(transpose_mask=False),\n                ])\n            else:\n                self.alb_transform = A.Compose([\n                    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n                    ToTensorV2(transpose_mask=False),\n                ])\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        if self.direct_volume:\n            # Build volume directly from series path\n            series_id = str(row['series_id'])\n            series_path = os.path.join(Config.SERIES_DIR, series_id)\n            arr = process_dicom_series_safe(series_path, target_shape=(32,384,384))\n            # Albumentations expects (H,W,C). Move channels to last then ToTensorV2 ‚Üí (C,H,W)\n            vol_hwc = np.transpose(arr, (1,2,0))  # (384,384,32)\n            out = self.alb_transform(image=vol_hwc)\n            image = out['image']  # tensor (32,384,384)\n            # Proactive memory cleanup of intermediates\n            try:\n                del vol_hwc\n                del arr\n            except Exception:\n                pass\n        else:\n            # Load ROI image\n            roi_path = row['roi_path']\n            try:\n                pil_img = Image.open(roi_path).convert('RGB')\n            except:\n                pil_img = Image.fromarray(np.random.randint(0, 255, (*Config.ROI_SIZE, 3), dtype=np.uint8))\n            np_img = np.array(pil_img)\n            try:\n                pil_img.close()\n            except Exception:\n                pass\n            out = self.alb_transform(image=np_img)\n            image = out['image']\n            try:\n                del np_img\n            except Exception:\n                pass\n        \n        # Get labels\n        labels = torch.tensor([row[col] for col in Config.LABEL_COLS], dtype=torch.float32)\n        \n        return {\n            'image': image,\n            'labels': labels,\n            'roi_id': row['roi_id'],\n            'confidence': torch.tensor(row['roi_confidence'], dtype=torch.float32)\n        }\n\nclass AneurysmEfficientNet(nn.Module):\n    \"\"\"EfficientNet-B3 (ROI mode) or EfficientNetV2-S (32-channel volume mode).\"\"\"\n    def __init__(self, num_classes=len(Config.LABEL_COLS)):\n        super().__init__()\n        self.direct_volume = getattr(Config, 'DIRECT_VOLUME_MODE', False)\n        if self.direct_volume:\n            # 32-channel EfficientNetV2-S\n            self.backbone = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=False, num_classes=0, in_chans=32)\n            feature_dim = self.backbone.num_features\n        else:\n            # ROI classifier (3-channel EfficientNet-B3) with offline weights\n            weights_path = '/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b3/1/tf_efficientnet_b3_aa-84b4657e.pth'\n            try:\n                self.backbone = timm.create_model('efficientnet_b3', pretrained=False, num_classes=0)\n                if os.path.exists(weights_path):\n                    print(f\"üîÑ Loading offline EfficientNet-B3 weights from: {weights_path}\")\n                    state_dict = torch.load(weights_path, map_location='cpu', weights_only=False)\n                    self.backbone.load_state_dict(state_dict, strict=False)\n                    print(\"‚úÖ Successfully loaded offline EfficientNet-B3 weights!\")\n                else:\n                    print(f\"‚ö†Ô∏è Weights file not found at {weights_path}, using random initialization\")\n            except Exception as e:\n                print(f\"‚ùå Error loading offline weights: {e}\")\n                print(\"üîÑ Falling back to timm without pre-training...\")\n                self.backbone = timm.create_model('efficientnet_b3', pretrained=False, num_classes=0)\n            feature_dim = self.backbone.num_features\n        \n        # Classification head with dropout\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x):\n        features = self.backbone(x)\n        logits = self.classifier(features)\n        return logits\n\n# Using original EfficientNet approach\n\ndef calculate_class_weights(df):\n    \"\"\"Calculate class weights with 13x multiplier for Aneurysm Present\"\"\"\n    pos_counts = df[Config.LABEL_COLS].sum()\n    neg_counts = len(df) - pos_counts\n    \n    # Standard frequency-based weights\n    class_weights = neg_counts / (pos_counts + 1e-8)\n    class_weights = np.minimum(class_weights, 100.0)  # Cap at 100\n    \n    # Apply 13x multiplier to \"Aneurysm Present\" (matches competition metric)\n    class_weights.iloc[-1] = class_weights.iloc[-1] * 13.0\n    \n    return torch.tensor(class_weights.values, dtype=torch.float32)\n\nprint(\"‚úÖ Model definition loaded\")\n\n# ====================================================\n# CELL 4: TRAINING PIPELINE\n# ====================================================\n\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    \n    for batch in tqdm(loader, desc=\"Training\"):\n        images = batch['image'].to(device, non_blocking=True)\n        labels = batch['labels'].to(device, non_blocking=True)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        with torch.cuda.amp.autocast(enabled=Config.MIXED_PRECISION):\n            images = images.to(memory_format=torch.channels_last)\n            logits = model(images)\n            loss = criterion(logits, labels)\n        \n        # Backward pass\n        if Config.MIXED_PRECISION:\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n            optimizer.step()\n        \n        total_loss += loss.item()\n        num_batches += 1\n    \n    return total_loss / num_batches\n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    num_batches = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Validating\"):\n            images = batch['image'].to(device, non_blocking=True)\n            labels = batch['labels'].to(device, non_blocking=True)\n            \n            with torch.cuda.amp.autocast(enabled=Config.MIXED_PRECISION):\n                images = images.to(memory_format=torch.channels_last)\n                logits = model(images)\n                loss = criterion(logits, labels)\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            # Collect predictions for AUC\n            probs = torch.sigmoid(logits).cpu().numpy()\n            all_preds.append(probs)\n            all_labels.append(labels.cpu().numpy())\n    \n    # Calculate AUC\n    if len(all_preds) > 0:\n        all_preds = np.vstack(all_preds)\n        all_labels = np.vstack(all_labels)\n        \n        try:\n            auc_scores = []\n            for i in range(len(Config.LABEL_COLS)):\n                if len(np.unique(all_labels[:, i])) > 1:\n                    auc = roc_auc_score(all_labels[:, i], all_preds[:, i])\n                    auc_scores.append(auc)\n                else:\n                    auc_scores.append(0.5)\n            \n            # Weighted AUC (13x weight for Aneurysm Present)\n            weights = [1.0] * (len(Config.LABEL_COLS) - 1) + [13.0]\n            weighted_auc = np.average(auc_scores, weights=weights)\n        except:\n            weighted_auc = 0.5\n    else:\n        weighted_auc = 0.5\n    \n    return total_loss / num_batches, weighted_auc\n\ndef main_training():\n    print(\"üöÄ STAGE 2: ANEURYSM CLASSIFICATION WITH EFFICIENTNET-B3\")\n    \n    # Load data\n    train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n    \n    if Config.DEBUG_MODE:\n        train_df = train_df.head(Config.DEBUG_SAMPLES)\n    \n    print(f\"Training samples: {len(train_df)}\")\n    print(f\"Aneurysm cases: {train_df['Aneurysm Present'].sum()}\")\n    \n    # Build training index (DIRECT_VOLUME_MODE: no ROIs)\n    records = []\n    for _, r in train_df.iterrows():\n        rec = {\n            'series_id': str(r[Config.ID_COL]),\n            'roi_id': f\"{str(r[Config.ID_COL])}_vol32\",\n            'roi_path': '',\n            'roi_confidence': 1.0,\n            'slice_idx': -1,\n        }\n        for col in Config.LABEL_COLS:\n            rec[col] = r[col]\n        records.append(rec)\n    training_df = pd.DataFrame(records)\n    \n    # Calculate class weights\n    class_weights = calculate_class_weights(training_df)\n    print(f\"Class weights: {class_weights}\")\n    \n    # Create criterion with class weights\n    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights).to(Config.DEVICE)\n    \n    # Mixed precision scaler\n    global scaler\n    scaler = torch.cuda.amp.GradScaler(enabled=Config.MIXED_PRECISION)\n    \n    # Cross-validation or single split\n    # Use Aneurysm Present for stratification\n    fold_scores = []\n    if Config.N_FOLDS <= 1:\n        idx_all = np.arange(len(training_df))\n        train_idx, val_idx = train_test_split(\n            idx_all,\n            test_size=0.2,\n            stratify=training_df['Aneurysm Present'],\n            random_state=42,\n        )\n        fold_splits = [(train_idx, val_idx)]\n    else:\n        skf = StratifiedKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=42)\n        fold_splits = list(skf.split(training_df, training_df['Aneurysm Present']))\n    \n    for fold, (train_idx, val_idx) in enumerate(fold_splits):\n        print(f\"\\n{'='*50}\")\n        print(f\"FOLD {fold + 1}/{Config.N_FOLDS}\")\n        print(f\"{'='*50}\")\n        \n        # Split data\n        train_fold_df = training_df.iloc[train_idx].reset_index(drop=True)\n        val_fold_df = training_df.iloc[val_idx].reset_index(drop=True)\n        \n        print(f\"Train samples: {len(train_fold_df)}, Val samples: {len(val_fold_df)}\")\n        \n        # Create datasets\n        train_dataset = AneurysmClassificationDataset(train_fold_df, mode='train')\n        val_dataset = AneurysmClassificationDataset(val_fold_df, mode='val')\n        \n        # Create loaders (tuned for throughput)\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=Config.BATCH_SIZE,\n            shuffle=True,\n            num_workers=Config.NUM_WORKERS,\n            pin_memory=Config.PIN_MEMORY,\n            persistent_workers=Config.PERSISTENT_WORKERS,\n            prefetch_factor=Config.PREFETCH_FACTOR,\n        )\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=Config.BATCH_SIZE,\n            shuffle=False,\n            num_workers=Config.NUM_WORKERS,\n            pin_memory=Config.PIN_MEMORY,\n            persistent_workers=Config.PERSISTENT_WORKERS,\n            prefetch_factor=Config.PREFETCH_FACTOR,\n        )\n        \n        # Initialize model (ensure in_chans=32 path is used)\n        model = AneurysmEfficientNet().to(Config.DEVICE)\n        try:\n            model = model.to(memory_format=torch.channels_last)\n        except Exception:\n            pass\n        \n        # Optimizer with different learning rates\n        optimizer = optim.AdamW([\n            {'params': model.backbone.parameters(), 'lr': Config.LEARNING_RATE * 0.1},  # Lower LR for backbone\n            {'params': model.classifier.parameters(), 'lr': Config.LEARNING_RATE}\n        ], weight_decay=1e-4)\n\n        # Multi-GPU if available\n        if torch.cuda.device_count() > 1:\n            model = nn.DataParallel(model)\n        \n        # Scheduler\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.EPOCHS)\n        \n        # Training loop\n        best_auc = 0\n        \n        for epoch in range(Config.EPOCHS):\n            print(f\"\\nEpoch {epoch+1}/{Config.EPOCHS}\")\n            \n            # Train\n            train_loss = train_epoch(model, train_loader, optimizer, criterion, Config.DEVICE)\n            \n            # Validate\n            val_loss, val_auc = validate_epoch(model, val_loader, criterion, Config.DEVICE)\n            \n            # Step scheduler\n            scheduler.step()\n            \n            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f}\")\n            \n            # Save best model\n            if val_auc > best_auc:\n                best_auc = val_auc\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_auc': val_auc,\n                    'epoch': epoch,\n                    'fold': fold\n                }, f'stage2_fold_{fold}_best.pth')\n                print(f\"üíæ Saved best model (AUC: {val_auc:.4f})\")\n        \n        fold_scores.append(best_auc)\n        print(f\"Fold {fold + 1} best AUC: {best_auc:.4f}\")\n    \n    # Final results\n    mean_cv_score = np.mean(fold_scores)\n    print(f\"\\n‚úÖ Cross-validation complete!\")\n    print(f\"Mean CV AUC: {mean_cv_score:.4f} ¬± {np.std(fold_scores):.4f}\")\n    print(f\"Individual fold scores: {fold_scores}\")\n\nprint(\"‚úÖ Training pipeline loaded\")\n\n# ====================================================\n# CELL 5: INFERENCE & SUBMISSION\n# ====================================================\n\nclass InferenceConfig:\n    \"\"\"Configuration for inference server\"\"\"\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    ID_COL = 'SeriesInstanceUID'\n    LABEL_COLS = [\n        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n    ]\n\nclass ModelEnsemble:\n    \"\"\"Ensemble of Stage 2 models for inference\"\"\"\n    def __init__(self, model_paths, device):\n        self.device = device\n        self.models = []\n        \n        # Replace model paths with your 5-fold 32ch checkpoints\n        base = '/kaggle/input/rsna2025-effnetv2-32ch'\n        model_paths = [\n            os.path.join(base, f'tf_efficientnetv2_s.in21k_ft_in1k_fold{i}_best.pth') for i in range(5)\n        ]\n        for path in model_paths:\n            try:\n                model = AneurysmEfficientNet().to(device)\n                checkpoint = torch.load(path, map_location=device, weights_only=False)\n                \n                if 'model_state_dict' in checkpoint:\n                    state_dict = checkpoint['model_state_dict']\n                else:\n                    state_dict = checkpoint\n                \n                # Handle DataParallel wrapper\n                if any(key.startswith('module.') for key in state_dict.keys()):\n                    state_dict = {key.replace('module.', ''): value for key, value in state_dict.items()}\n                \n                model.load_state_dict(state_dict)\n                model.eval()\n                self.models.append(model)\n                print(f\"Loaded model: {path}\")\n            except Exception as e:\n                print(f\"Error loading {path}: {e}\")\n        \n        print(f\"Loaded {len(self.models)} models for ensemble\")\n    \n    def predict_single(self, series_path):\n        \"\"\"Predict by building a 32x384x384 volume directly from DICOMs using shared preprocessor.\"\"\"\n        vol = process_dicom_series_safe(series_path, target_shape=(32,384,384))\n        x = torch.from_numpy(vol).unsqueeze(0).to(self.device)  # [1,32,384,384]\n        preds = []\n        with torch.no_grad():\n            for model in self.models:\n                logits = model(x)\n                probs = torch.sigmoid(logits).cpu().numpy()[0]\n                preds.append(probs)\n        return np.mean(np.stack(preds, axis=0), axis=0)\n\nclass InferenceDICOMProcessor:\n    \"\"\"DICOM processor for inference\"\"\"\n    def __init__(self):\n        pass\n\n# Global variables for model ensemble\nmodel_ensemble = None\nprocessor = None\n\ndef initialize_models():\n    \"\"\"Initialize models - called once at startup\"\"\"\n    global model_ensemble, processor\n    \n    print(\"Initializing models...\")\n    \n    # Model paths - adjust these to match your uploaded dataset structure\n    model_paths = [\n        'stage2_fold_0_best.pth',\n        'stage2_fold_1_best.pth',\n        'stage2_fold_2_best.pth',\n        'stage2_fold_3_best.pth',\n        'stage2_fold_4_best.pth',\n    ]\n    \n    # Check if models exist, use available ones\n    available_models = [path for path in model_paths if os.path.exists(path)]\n    \n    if not available_models:\n        print(\"Warning: No trained models found! Using dummy predictions.\")\n        model_ensemble = None\n    else:\n        try:\n            model_ensemble = ModelEnsemble(available_models, InferenceConfig.DEVICE)\n            print(\"Models initialized successfully!\")\n        except Exception as e:\n            print(f\"Error initializing models: {e}\")\n            model_ensemble = None\n    \n    processor = InferenceDICOMProcessor()\n\ndef predict(series_path: str) -> pl.DataFrame:\n    \"\"\"Make predictions for the competition API\"\"\"\n    global model_ensemble, processor\n    \n    # Initialize models on first call (lazy loading)\n    if model_ensemble is None and processor is None:\n        initialize_models()\n    \n    series_id = os.path.basename(series_path)\n    \n    try:\n        if model_ensemble is not None:\n            # Use trained ensemble\n            predictions = model_ensemble.predict_single(series_path)\n        else:\n            # Fallback: extract metadata and make informed dummy predictions\n            print(f\"Using fallback prediction for {series_id}\")\n            \n            # Load DICOM metadata\n            all_filepaths = []\n            for root, _, files in os.walk(series_path):\n                for file in files:\n                    if file.endswith('.dcm'):\n                        all_filepaths.append(os.path.join(root, file))\n            \n            if all_filepaths:\n                ds = pydicom.dcmread(all_filepaths[0], force=True)\n                modality = getattr(ds, 'Modality', 'UNKNOWN')\n                \n                # Slightly better informed predictions based on modality\n                if modality in ['CTA', 'MRA']:\n                    # Vascular imaging - slightly higher probability\n                    base_prob = 0.1\n                else:\n                    # Other modalities - lower baseline\n                    base_prob = 0.05\n                \n                # Add some noise to make predictions more realistic\n                predictions = np.random.normal(base_prob, 0.02, len(InferenceConfig.LABEL_COLS))\n                predictions = np.clip(predictions, 0.001, 0.999)\n            else:\n                # No DICOM files found\n                predictions = np.full(len(InferenceConfig.LABEL_COLS), 0.5)\n\n        # Ensure predictions is numpy array and convert to list safely\n        if not isinstance(predictions, np.ndarray):\n            predictions = np.array(predictions)\n        \n        # Create prediction DataFrame\n        prediction_df = pl.DataFrame(\n            data=[[series_id] + predictions.tolist()],\n            schema=[InferenceConfig.ID_COL, *InferenceConfig.LABEL_COLS],\n            orient='row',\n        )\n        \n    except Exception as e:\n        print(f\"Error processing {series_id}: {e}\")\n        # Return safe default predictions\n        prediction_df = pl.DataFrame(\n            data=[[series_id] + [0.5] * len(InferenceConfig.LABEL_COLS)],\n            schema=[InferenceConfig.ID_COL, *InferenceConfig.LABEL_COLS],\n            orient='row',\n        )\n    \n    # IMPORTANT: Remove SeriesInstanceUID before returning (API requirement)\n    prediction_df = prediction_df.drop(InferenceConfig.ID_COL)\n    \n    # IMPORTANT: Disk cleanup to prevent \"out of disk space\" errors\n    shutil.rmtree('/kaggle/shared', ignore_errors=True)\n    \n    return prediction_df\n\n\n\n# ====================================================\n# SERVER EXECUTION\n# ====================================================\n\n# Initialize the inference server\ninference_server = kaggle_evaluation.rsna_inference_server.RSNAInferenceServer(predict)\n\nprint(\"‚úÖ Inference and submission pipeline loaded\")\n\n# ====================================================\n# CELL 6: MAIN EXECUTION\n# ====================================================\n\nif __name__ == \"__main__\":\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        # Production mode - serve the API\n        print(\"Starting inference server...\")\n        inference_server.serve()\n    else:\n        # Training mode\n        print(\"Ready for Stage 2 training!\")\n        print(\"Uncomment the line below to start training:\")\n        print(\"# main_training()\")\n        \n        # Uncomment to start training\n        main_training()\n        \n        # Or run local testing\n        print(\"Running local gateway for testing...\")\n        inference_server.run_local_gateway()\n        \n        # Display results if available\n        results_path = '/kaggle/working/submission.parquet'\n        if os.path.exists(results_path):\n            results_df = pl.read_parquet(results_path)\n            print(\"Submission preview:\")\n            print(results_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T07:20:11.145979Z","iopub.execute_input":"2025-09-07T07:20:11.146469Z","execution_failed":"2025-09-07T07:24:04.277Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Configuration loaded - Device: cuda\n‚úÖ Data loading and ROI extraction functions loaded\n‚úÖ Model definition loaded\n‚úÖ Training pipeline loaded\n‚úÖ Inference and submission pipeline loaded\nReady for Stage 2 training!\nUncomment the line below to start training:\n# main_training()\nüöÄ STAGE 2: ANEURYSM CLASSIFICATION WITH EFFICIENTNET-B3\nTraining samples: 4348\nAneurysm cases: 1864\nClass weights: tensor([54.7436, 43.3673, 12.1360, 14.6968, 18.8539, 13.7891, 10.9780, 93.5217,\n        76.6429, 49.5581, 42.0495, 38.5273, 37.4779, 17.3240])\n\n==================================================\nFOLD 1/1\n==================================================\nTrain samples: 3478, Val samples: 870\n\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/1739 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.27680122398901436027276783658914589954_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.20352838605781624312895197978664744075_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12904246053955178641505906243733756576_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.10783586076403918900057381253415239230_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.13035832792413871820010907388005791076_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11231019858377850021999891102731187707_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.10092666779602341135460882241562348436_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.25401566480135645158545753333376825827_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.14727653378259316193461820952319318524_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.34802588129024719779720167752927019812_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.58031681175140394217101072855030384165_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.26176605961582684334066705106375230446_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.48131866374867671121851712076545426103_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.10557979063651009599662513943433444820_32x384x384_u8.npy (max_err=0.0020)[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.44342648322577340441651380684542219719_32x384x384_u8.npy (max_err=0.0020)\n\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.13277962627210317866440333144841492664_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.54995046890244844619428693288281206240_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.58451292451520550834467596828575726648_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 1/1739 [00:36<17:26:50, 36.14s/it]","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.89460822484126633248553997073630753402_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12743083402126679385964805363054623625_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 6/1739 [00:39<1:12:13,  2.50s/it] ","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11114613141735642199606043212646844886_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.86037975393556827852769300088670915080_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.99421822954919332641371697175982753182_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.99297218927715340305099097057004586774_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.31628002870565033361286640405875848972_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.15847980512533357707448321523314296455_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.97191581480677904605690881854234107618_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.59321031989170539770517544048571714746_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.88727133389627156951186577049810067833_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11198791437802468548828730795882522615_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12510021855520344031681088163018665763_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11924598622382686493858949897461757527_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.45274348753893515110759637950992425503_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.46323212394021990293051146253964627947_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12273455431673968216551570318744317465_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12305532574743349097112160025098568126_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.66863687043210829873963610390708568451_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12337922590270748937219070427392744694_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.96758662280979863201306643425532251628_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.39399936216847450890619266058595478036_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11098780782340815507287784394577673659_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.13339453702287018579729252048707127389_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.96937347732144416459815687382646883895_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.46989411517160437881185602824099566545_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.99895756899933461331612313434452838359_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|          | 12/1739 [01:12<59:20,  2.06s/it]  ","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.76301795615527602645756396708867809495_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.49504988543101147636991852267737251575_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.87865223651281657051189368725400341319_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.90230341788943218278385841963462570470_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.45789072046383277170393600308966109036_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.95380253040471768084221411882180922662_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.53947155422591684879953627516013605305_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.30885835981120543326208883457853128283_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12657866377259426932577960725887441807_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.77710080228621144369590367259303137264_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.33846340251863147853963750710189797262_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.81181812554261337166445969981198673971_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.42684225889151412572900157020640134612_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.44549497440423264593272188259201743021_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.43668951483751128693751511357612846073_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.67387348212182171367128687078028809410_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.10594694793170397064169815033438514439_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.46822812430410322726611601181306798493_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.10302299037333930209177350775866905985_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|          | 15/1739 [01:49<2:56:00,  6.13s/it]","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.85212589540007626039427792519492210226_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|          | 18/1739 [01:50<1:03:35,  2.22s/it]","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11948255979244132827019816539294376988_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.46462519342058199786903141190024113863_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|          | 21/1739 [01:53<36:51,  1.29s/it]  ","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.71550538308484373791005892758892068095_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|‚ñè         | 24/1739 [01:54<18:27,  1.55it/s]","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.28681157493123082643438198449009757076_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.52524639115355387664045096288385299391_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.13156819427968405341920939838729113222_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11731089624678785415420487370578919131_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11062397380277678777080157173387177272_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11208788596258922886794998326857227331_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11038636852681039246443401046449812061_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.10603321067992496978932502160661673268_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.16619890023070193783587383755389255978_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.89230355061878888016429746695193995605_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.33700500631944960587614822567849387920_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.21666801890966359261094968251234100656_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12803193618276623824221823905815029240_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12768843892616555302989444677286558651_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.39385992993593136245408779480147617316_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.74444271188570856914042690321209734730_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.81593436202151422508510092765980279560_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.14684036027519224245993347612595868123_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.45424805947272697393885330708447229632_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.47198997109856483529015070075855120913_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.10215833141558976135001043369327881438_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11968949928784170488927502358577806155_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.29409982658103845473759110643077681780_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12979142503868963701242534180745551377_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.58611233316672173480399067136043036711_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12073660042268180923540565125477883696_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.20979964870558122513140407598441297537_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   2%|‚ñè         | 30/1739 [02:30<58:24,  2.05s/it]  ","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.17650328348608009816816941699740585437_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.85330951120080333123485292655736144682_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12821030325057451794033542804285567094_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.19950322290309930502685963351749767205_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.89561322985962991141463885723229681301_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.13152457913019434787350387410585992407_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.53573089697219772119994230159413715556_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   2%|‚ñè         | 34/1739 [02:35<31:21,  1.10s/it]  ","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11873351578622765241634317263552561587_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   2%|‚ñè         | 36/1739 [02:36<19:02,  1.49it/s]","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.31016492921636257021969319428153307687_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   2%|‚ñè         | 42/1739 [02:39<10:48,  2.62it/s]","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.55335626170859465016687259915777364744_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.71583117639965131882497910550331790290_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.72305112536340538867034966246953618485_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.29667087068052601737556059884413817393_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.26383584662098611508554214963067859078_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   3%|‚ñé         | 47/1739 [02:47<19:53,  1.42it/s]  ","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11772545330652739508075303939268792529_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.99953513260518059135058337324142717073_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.77351519132509988103103734443501529160_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   3%|‚ñé         | 48/1739 [02:50<42:01,  1.49s/it]","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.23421600482463782319293054087843086911_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.60806471972978535805998258895959371616_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.70048682207873090597326950007352492114_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.19764176435911045852235280876942035947_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.29789030841471927572567470683240960289_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   3%|‚ñé         | 51/1739 [02:59<50:21,  1.79s/it]  ","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.53713028846939619414399725027946568503_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   3%|‚ñé         | 53/1739 [02:59<28:20,  1.01s/it]","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.38094808038974181102880321183103989801_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.90272546526306161811446757328579665073_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12159152010278655162358172837938626290_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.35462271463152990781312639766446467244_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11776450499172121144481170405958665580_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.10097649530131165889513682791963111629_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.11864645671097263388176300581289300776_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.61225069775477744578753822969424698697_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.55948936024722340193987053115675558858_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.71551553792563365338142343263415112514_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.42761711172547175180367207934640355283_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.89158384270238840672537034882728290533_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   3%|‚ñé         | 54/1739 [03:18<2:54:13,  6.20s/it]","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.28710895896233158724073271531642622364_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"},{"name":"stderr","text":"Training:   3%|‚ñé         | 59/1739 [03:18<33:05,  1.18s/it]  ","output_type":"stream"},{"name":"stdout","text":"[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12813565901564977994662924864827111603_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.14234941301612013649573263693853357171_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.18108212083513041239064199663549795472_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.32738278165208105984060645831271331150_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.82754434126210061881442049561952688899_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.79053237532664154618488686227121698456_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.33254059742616938664293801285152925743_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.23657152176763679599021789757461301944_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12016612089939417546691633902379080729_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.34932758595723144931370662768062741526_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.75099399846524353039262702677039716253_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.75364570250169818831646925655404227168_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.19295797161815835738078824460590782108_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.35518556574512517572156607568659172794_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.68643319369651654844141433329820410384_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.72965878451742697760874838447353231989_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.61153322328776313915781838928080962114_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.74390569791112039529514861261033590424_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.13112649411925356765453279717965661997_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12829395253364827707081791959083813349_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.67758788374759493339640568043304111456_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.91747317296331896239489506267037968773_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.10960467202537249314034213241421770874_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.91483546770441793632728100550401143080_32x384x384_u8.npy (max_err=0.0020)\n[CACHE] wrote u8: 1.2.826.0.1.3680043.8.498.12132622846836853200891705613461466627_32x384x384_u8.npy (max_err=0.0020)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}